{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661de216-bc8b-476c-8bcd-d7a5faeac7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbd689-0734-48f3-b2bf-7cdc17bba79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f431815-c0d0-4314-b8dd-dfb30937f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae431d0-3ac1-4ccf-8ea1-b4be0bb3e08b",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "- https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31b639-a09e-4186-bd4f-f4a3a37a6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = nlp('man')\n",
    "word2 = nlp('woman')\n",
    "word3 = nlp('king')\n",
    "word4 = nlp('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1291c-4315-4b67-b160-0f8b6778effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = word3.vector - word1.vector + word2.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a689ae0-2faa-474a-95c9-c2cb4dbf428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = np.dot(test, word4.vector) / (np.linalg.norm(test) * np.linalg.norm(word4.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7bc33-18d9-4ad4-9499-1c380ff7d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5137ea-99c0-4978-aabf-86d0782264a7",
   "metadata": {},
   "source": [
    "## Linguistic Annotations and Features\n",
    "- without underscore, hash values\n",
    "- https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732d2fe-8d85-4be6-b2cd-ef16ed7cbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K.'s startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f576c-beee-4380-953e-12e8fa627a2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Part of speech tagging, morphology, lemmatization, and entity linking\n",
    "- parse trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7387f-a00c-4eb6-9b92-750d43fcd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for token in doc:\n",
    "    lst.append(\n",
    "        [\n",
    "            token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop, token.morph,\n",
    "            token.head.text, token.head.pos_, token.ent_iob, token.ent_iob_, token.ent_type_,\n",
    "            token.ent_kb_id_, [child for child in token.children]\n",
    "        ]\n",
    "    )\n",
    "cols = [\n",
    "    'text', 'lemma', 'pos', 'tag', 'dep', \n",
    "    'shape', 'alpha', 'stop', 'morph', \n",
    "    'head text', 'head pos', 'ent iob', 'ent iob_', 'ent type',\n",
    "    'kb id', 'children'\n",
    "]\n",
    "df_token = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ba061-732a-4cec-b876-288e4f240540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf016e5d-8c7b-43e5-af8a-dcaedd626fcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edddb6-ca37-4d02-9fcc-38df1ad979c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun chunks\n",
    "lst = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    lst.append(\n",
    "        [\n",
    "            chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text\n",
    "        ]\n",
    "    )\n",
    "cols = ['text', 'root text', 'root dep', 'root head text']\n",
    "df_chunk = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148acadc-2f38-4e11-b833-b138b6860a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9128dd-6c0b-4a8d-8396-65697f508bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a verb with a subject\n",
    "nsubj = spacy.symbols.nsubj\n",
    "VERB = spacy.symbols.VERB\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f09fe-ee25-418c-84ab-134f4fa73a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating around a local tree\n",
    "word = doc[2]\n",
    "doc_lefts = [token.text for token in word.lefts]\n",
    "doc_rights = [token.text for token in word.rights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1d679-d1ec-481c-83a0-68ae663d901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Word:', word)\n",
    "print (f'Doc lefts ({word.n_lefts}): {doc_lefts}')\n",
    "print (f'Doc rights ({word.n_rights}): {doc_rights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2af27d-2b89-4dbe-a85d-5b3cb5707e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5a9d5-3b18-4a5f-b23b-b120d01d6803",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Credit example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b6c49-b276-4152-996c-67a7367a4403",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_credit = nlp(\"Credit and mortgage account holders must submit their requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cfbc1-6898-4cdf-b2c1-f6986cfe352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ancestors in subtree\n",
    "root = [token for token in doc_credit if token.head == token][0]\n",
    "subject = list(root.lefts)[0]\n",
    "lst = []\n",
    "for descendant in subject.subtree:\n",
    "    assert subject is descendant or subject.is_ancestor(descendant)\n",
    "    lst.append(\n",
    "        [\n",
    "            descendant.text, descendant.dep_, descendant.n_lefts, descendant.n_rights,\n",
    "            [ancestor.text for ancestor in descendant.ancestors]\n",
    "        ]\n",
    "    )\n",
    "cols = ['text', 'dep', 'n lefts', 'n rights', 'ancestors']\n",
    "df_subtree = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace7775-d14e-4a09-bd65-4bb05932fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54564fb4-3b13-4938-9807-d0319aeb7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use edges to create a span\n",
    "span = doc_credit[doc_credit[4].left_edge.i : doc_credit[4].right_edge.i+1]\n",
    "with doc_credit.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "lst = []\n",
    "for token in doc_credit:\n",
    "    lst.append(\n",
    "        [\n",
    "            token.text, token.pos_, token.dep_, token.head.text\n",
    "        ]\n",
    "    )\n",
    "cols = ['text', 'pos', 'dep', 'head text']\n",
    "df_span = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fdf1a-bcd1-46a5-8679-db2f17ce35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de29049-6b4a-43cf-8f9c-f5d340912cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually extract information from text\n",
    "# Merge noun phrases and entities for easier analysis\n",
    "#nlp.add_pipe(\"merge_entities\")\n",
    "#nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "TEXTS = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "for doc_text in nlp.pipe(TEXTS):\n",
    "    for token in doc_text:\n",
    "        if token.ent_type_ == \"MONEY\":\n",
    "            # We have an attribute and direct object, so check for subject\n",
    "            if token.dep_ in (\"attr\", \"dobj\"):\n",
    "                subj = [w for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "                if subj:\n",
    "                    print(subj[0], \"-->\", token)\n",
    "            # We have a prepositional object with a preposition\n",
    "            elif token.dep_ == \"pobj\" and token.head.dep_ == \"prep\":\n",
    "                print(token.head.head, \"-->\", token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3461090-ae24-4340-b6ce-21b48e12bce9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd47fe-20b8-464d-ae0d-758f217f2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for ent in doc.ents:\n",
    "    lst.append(\n",
    "        [\n",
    "            ent.text, ent.start_char, ent.end_char, ent.label_, ent.kb_id_\n",
    "        ]\n",
    "    )\n",
    "cols = ['text', 'start', 'end', 'label', 'knowledge base id']\n",
    "df_ent = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abdcba-7286-452b-9039-24299088cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c9931-692d-4230-83d7-b163312ab44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001375f0-8b07-4e90-b6ba-22500dd3615c",
   "metadata": {},
   "source": [
    "#### FB example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f784f-c748-4e32-953d-086c11f6d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fb as an entity\n",
    "doc_fb = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc_fb.ents]\n",
    "print('Before', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :(\n",
    "\n",
    "# Create a span for the new entity\n",
    "fb_ent = spacy.tokens.Span(doc_fb, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc_fb.ents)\n",
    "\n",
    "# Option 1: Modify the provided entity spans, leaving the rest unmodified\n",
    "doc_fb.set_ents([fb_ent], default=\"unmodified\")\n",
    "\n",
    "# Option 2: Assign a complete list of ents to doc.ents\n",
    "doc_fb.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start, e.end, e.label_) for e in doc_fb.ents]\n",
    "print('After', ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8a95b-7e03-463a-b06b-3c8bfa437de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fb as an entity using arrays\n",
    "doc_fb = nlp.make_doc(\"fb is hiring a new vice president of global policy\")\n",
    "print(\"Before\", doc_fb.ents)  # []\n",
    "\n",
    "header = [spacy.attrs.ENT_IOB, spacy.attrs.ENT_TYPE]\n",
    "attr_array = np.zeros((len(doc_fb), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc_fb.vocab.strings[\"ORG\"]\n",
    "doc_fb.from_array(header, attr_array)\n",
    "print(\"After\", doc_fb.ents)  # [London]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88157efc-65af-4d43-be12-5daa141bc274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb985e9d-97f4-4934-bc35-001eb54e3be9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- customizing tokenizer class\n",
    "- modify existing rule sets\n",
    "- building a basic white space tokenizer\n",
    "- using third party tokenizers (e.g. BERT)\n",
    "- training with a custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e560076-8ef8-4c4b-925d-3dbeadb5a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special case rule\n",
    "ORTH = spacy.symbols.ORTH\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"...gimme! that\")])  # ['gim', 'me', 'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7d1ba-cc1e-4cf6-bb3d-d866e8f67d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer.explain(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde95fd-4b2e-48a4-bb5c-7d26552363d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.prefixes[:10], nlp.Defaults.suffixes[:10], nlp.Defaults.infixes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210e61c-2d1f-4724-a9e9-bad5797b9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.Defaults.prefixes), len(nlp.Defaults.suffixes), len(nlp.Defaults.infixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58d5db-cd28-41f3-aed3-6827720b3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Hello\", \",\", \"world\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "doc_from_words = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc_from_words.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f20af4-7f5c-4a41-835c-3fe5426cfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'\", \"s\", \"podcasts\", \".\"]\n",
    "spacy_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'s\", \"podcasts\", \".\"]\n",
    "align = spacy.training.Alignment.from_strings(other_tokens, spacy_tokens)\n",
    "print(f\"a -> b, lengths: {align.x2y.lengths}\")  # array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "print(f\"a -> b, mapping: {align.x2y.data}\")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to \"'s\"\n",
    "print(f\"b -> a, lengths: {align.y2x.lengths}\")  # array([1, 1, 1, 1, 2, 1, 1])   : the token \"'s\" refers to two tokens\n",
    "print(f\"b -> a, mappings: {align.y2x.data}\")  # array([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993d870-33d5-49b1-8530-fc8df9164df1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Merging and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc204a-55b0-44f6-a029-3acac0d63c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ny = nlp(\"I live in New York\")\n",
    "print(\"Before:\", [token.text for token in doc_ny])\n",
    "spacy.displacy.render(doc_ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954ac93-97bc-4f35-9519-8d4a910d2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc_ny.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc_ny[3:5], attrs={\"LEMMA\": \"new york\"})\n",
    "print(\"After:\", [token.text for token in doc_ny])\n",
    "spacy.displacy.render(doc_ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11ed53-92c2-43a1-b0c4-5a45f4aa7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ny = nlp(\"I live in NewYork\")\n",
    "print(\"Before:\", [token.text for token in doc_ny])\n",
    "spacy.displacy.render(doc_ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934778a-076d-4a83-81aa-91568969d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc_ny.retokenize() as retokenizer:\n",
    "    heads = [(doc_ny[3], 1), doc_ny[2]]\n",
    "    attrs = {\"POS\": [\"PROPN\", \"PROPN\"], \"DEP\": [\"pobj\", \"compound\"]}\n",
    "    retokenizer.split(doc_ny[3], [\"New\", \"York\"], heads=heads, attrs=attrs)\n",
    "print(\"After:\", [token.text for token in doc_ny])\n",
    "spacy.displacy.render(doc_ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910200de-c80a-4f25-a94e-521d35172ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ny = nlp(\"I live in NewYork\")\n",
    "with doc_ny.retokenize() as retokenizer:\n",
    "    heads = [(doc_ny[3], 0), (doc_ny[3], 1)]\n",
    "    retokenizer.split(doc_ny[3], [\"New\", \"York\"], heads=heads)\n",
    "print(\"After:\", [token.text for token in doc_ny])\n",
    "spacy.displacy.render(doc_ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc16e9e-aa68-4385-99c5-8ea92dc36f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.tokens.Token.set_extension(\"is_city\", default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410ff2e-9c52-4252-adbc-89e01d42e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ny = nlp(\"I live in New York\")\n",
    "print(\"Before:\", [(token.text, token._.is_city) for token in doc_ny])\n",
    "\n",
    "with doc_ny.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc_ny[3:5], attrs={\"_\": {\"is_city\": True}})\n",
    "print(\"After:\", [(token.text, token._.is_city) for token in doc_ny])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e13184-fd32-4cf2-a256-527174bf074a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c7645-db47-4cd6-8abf-b3fc0f00d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency parse (default)\n",
    "doc_sent = nlp(\"This is a sentence. This is another sentence.\")\n",
    "print ('Doc has sent start anotation:', doc_sent.has_annotation(\"SENT_START\"))\n",
    "for sent in doc_sent.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f7f13-4b1e-43e1-80fc-f4caa7ca03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical segmenter\n",
    "nlp_sent = spacy.load(\"en_core_web_lg\", exclude=[\"parser\"])\n",
    "nlp_sent.enable_pipe(\"senter\")\n",
    "doc_sent = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc_sent.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a4ab4-5da7-422b-bf8f-62830272aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based pipeline\n",
    "nlp_sent = spacy.lang.en.English()  # just the language with no pipeline\n",
    "nlp_sent.add_pipe(\"sentencizer\")\n",
    "doc_sent = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc_sent.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41d3e5-0d93-4143-86ec-566a93de022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom rule based\n",
    "text = \"this is a sentence...hello...and another sentence.\"\n",
    "\n",
    "nlp_sent = spacy.load(\"en_core_web_lg\")\n",
    "doc_sent = nlp(text)\n",
    "print(\"Before:\", [sent.text for sent in doc_sent.sents])\n",
    "\n",
    "@spacy.language.Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp_sent.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "doc_sent = nlp_sent(text)\n",
    "print(\"After:\", [sent.text for sent in doc_sent.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2e1de-5fe3-40fe-98c3-080d0fbb0cf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mappings and exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc947f-a3ec-4d4e-a53e-74646e8ca713",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_who = spacy.load(\"en_core_web_lg\")\n",
    "text = \"I saw The Who perform. Who did you see?\"\n",
    "doc_who = nlp_who(text)\n",
    "print(doc_who[2].tag_, doc_who[2].pos_)  # DT DET\n",
    "print(doc_who[3].tag_, doc_who[3].pos_)  # WP PRON\n",
    "\n",
    "# Add attribute ruler with exception for \"The Who\" as NNP/PROPN NNP/PROPN\n",
    "ruler = nlp_who.get_pipe(\"attribute_ruler\")\n",
    "# Pattern to match \"The Who\"\n",
    "patterns = [[{\"LOWER\": \"the\"}, {\"TEXT\": \"Who\"}]]\n",
    "# The attributes to assign to the matched token\n",
    "attrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n",
    "# Add rules to the attribute ruler\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=0)  # \"The\" in \"The Who\"\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Who\" in \"The Who\"\n",
    "\n",
    "doc_who_ruler = nlp_who(text)\n",
    "print(doc_who_ruler[2].tag_, doc_who_ruler[2].pos_)  # NNP PROPN\n",
    "print(doc_who_ruler[3].tag_, doc_who_ruler[3].pos_)  # NNP PROPN\n",
    "# The second \"Who\" remains unmodified\n",
    "print(doc_who_ruler[5].tag_, doc_who_ruler[5].pos_)  # WP PRON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4c0ff-d2ad-4b12-847d-5f7cf9748f90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Vectors and Similarity\n",
    "- similarity is subjective\n",
    "- sentence embeddings for words: mean vector of words (i.e. insensitive to order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d8ff9-37c6-46a3-9686-4592a4aae3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp('dog cat banana afskfsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b1932-d857-4dca-a577-c95191e73509",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for token in tokens:\n",
    "    lst.append(\n",
    "        [\n",
    "            token.text, token.has_vector, token.vector_norm, token.is_oov\n",
    "        ]\n",
    "    )\n",
    "cols = ['texx', 'vector', 'norm', 'oov']\n",
    "df_vec = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a657bb-59a6-43bd-ad8d-5a062eaded8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0694feb-d925-4a1c-ade4-dd5def8f14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('I like salty fries and hamburgers.')\n",
    "doc2 = nlp('Fast food tastes very good.')\n",
    "print ('Sentence similarity:', doc1.similarity(doc2))\n",
    "print ('Word similarity:', doc1[2:4].similarity(doc1[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42c2bb-0d57-4c9d-84f9-7d24223bb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_nouns = nlp(' '.join([str(t) for t in doc1 if t.pos_ in ['NOUN', 'PROPN']]))\n",
    "doc2_nouns = nlp(' '.join([str(t) for t in doc2 if t.pos_ in ['NOUN', 'PROPN']]))\n",
    "print ('Doc1:', doc1_nouns)\n",
    "print ('Doc2:', doc2_nouns)\n",
    "print ('Similarity:', doc1_nouns.similarity(doc2_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d961c80-17a5-4b72-ae8e-4068f69bf005",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_no_stop_words = nlp(' '.join([str(t) for t in doc1 if not t.is_stop]))\n",
    "doc2_no_stop_words = nlp(' '.join([str(t) for t in doc2 if not t.is_stop]))\n",
    "print ('Doc1:', doc1_no_stop_words)\n",
    "print ('Doc2:', doc2_no_stop_words)\n",
    "print ('Similarity:', doc1_no_stop_words.similarity(doc2_no_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d2895-8b67-4939-a19b-27c59b70a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data = {\n",
    "    \"dog\": np.random.uniform(-1, 1, (300,)),\n",
    "    \"cat\": np.random.uniform(-1, 1, (300,)),\n",
    "    \"orange\": np.random.uniform(-1, 1, (300,))\n",
    "}\n",
    "\n",
    "vocab_new = spacy.vocab.Vocab()\n",
    "for word, vector in vector_data.items():\n",
    "    vocab_new.set_vector(word, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a91ebd-7075-40e1-893d-c8ca218475ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pipelines, Architecture, Serialization, Training, and Language data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bdec5-c22e-44ec-bfda-051ff160377b",
   "metadata": {},
   "source": [
    "- text nlp doc\n",
    "  - text (tokenizer processing pipeline) doc\n",
    "  - text (tokenizer (tagger parser ner lemmatizer textcat custom)) doc\n",
    "- doc, docbin, example, language, lexeme, span, spangroup, token\n",
    "- creating and registering custom language subclass (e.g. adding stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cc24d-2a53-413e-a8c2-5fc324de5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for token in doc:\n",
    "    lexeme = doc.vocab[token.text]\n",
    "    lst.append(\n",
    "        [\n",
    "            lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_\n",
    "        ]\n",
    "    )\n",
    "cols = ['text', 'orth', 'shape', 'prefix', 'suffix', 'alpha', 'digit', 'title', 'lang']\n",
    "df_lex = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11775890-65ab-43ab-819f-238b5c214fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9db00-1fde-49d1-b6fd-54b9d3920146",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_hash = nlp.vocab.strings['apple']\n",
    "apple_str = nlp.vocab.strings[apple_hash]\n",
    "apple_hash, apple_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016ac68-bdcd-4098-ae0d-11b93dc03dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnglishDefaults(spacy.lang.en.English.Defaults):\n",
    "    stop_words = set([\"custom\", \"stop\"])\n",
    "\n",
    "class CustomEnglish(spacy.lang.en.English):\n",
    "    lang = \"custom_en\"\n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "nlp1 = spacy.lang.en.English()\n",
    "nlp2 = CustomEnglish()\n",
    "\n",
    "print(nlp1.lang, [token.is_stop for token in nlp1(\"custom stop\")])\n",
    "print(nlp2.lang, [token.is_stop for token in nlp2(\"custom stop\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49406cdb-54dd-4a12-abf6-0ad282035729",
   "metadata": {},
   "source": [
    "## Sentence Encoders\n",
    "- Universal sentence encoder via [spacy](https://spacy.io/universe/project/spacy-universal-sentence-encoder) and [tensorflow](https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder)\n",
    "- [Sentence transformers](https://huggingface.co/sentence-transformers)\n",
    "- Can also embed paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84eb0e3-7bbe-4eaa-9313-7b295db73a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy_universal_sentence_encoder\n",
    "\n",
    "#nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')\n",
    "#doc = nlp(\"This is a test sentence.\")\n",
    "#print(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4ed99-40ea-4671-a877-1ffd2bd9cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_hub as hub\n",
    "#import tensorflow as tf\n",
    "\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "#embeddings = embed([\"This is a test sentence.\", \"And another one.\"])\n",
    "#print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f82d9-25d6-4b49-9570-36481e74f992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
